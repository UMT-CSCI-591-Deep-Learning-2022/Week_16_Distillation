{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e198285",
   "metadata": {},
   "source": [
    "# Knowledge Distillation on MNIST\n",
    "Knowledge distillation is the process of transferring the higher performance of a more expensive model to a smaller one.  In this notebook, we will explore performing this process on MNIST.  To begin with, I have provided access to pre-trained model that is large, but performant.  The exact architecture is not relevant (although you can inspect this easily if you wish).  It is straightforward to load in pytorch with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bef4534c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (l1): Linear(in_features=784, out_features=800, bias=True)\n",
      "  (l2): Linear(in_features=800, out_features=800, bias=True)\n",
      "  (l3): Linear(in_features=800, out_features=10, bias=True)\n",
      "  (dropout2): Dropout(p=0.5, inplace=False)\n",
      "  (dropout3): Dropout(p=0.5, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = 'cpu'\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.l1 = torch.nn.Linear(28**2,800)\n",
    "        self.l2 = torch.nn.Linear(800,800)\n",
    "        self.l3 = torch.nn.Linear(800,10)\n",
    "        self.dropout2 = torch.nn.Dropout(0.5)\n",
    "        self.dropout3 = torch.nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.l1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.l2(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.dropout3(x)\n",
    "        x = self.l3(x)\n",
    "        return x\n",
    "    \n",
    "big_model = torch.load('pretrained_model.pt').to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78179abb",
   "metadata": {},
   "source": [
    "First, let's establish the baseline performance of the big model on the MNIST test set.  Of course we'll need acces to the MNIST test set to do this.  At the same time, let's also get our transfer set, which in this case will be a $n=10k$ subset of the full MNIST training set (using a subset is helpful for speeding up training of distilled models, and also helps showcase some of the improved performance due to model distillation).   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "177a7acf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2859248524bd4e78bfb68431597489a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9912422 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70535f4f9cd1464a88c7cf8370522272",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28881 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95d46e1558c94aafae799cabb2a8dd16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1648877 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad334829bfb24fc0a7591ca77f2cc1c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4542 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2036d3f6f5134dffb95fb4ba9de60180",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9912422 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data\\MNIST\\raw\\train-images-idx3-ubyte.gz to ../data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "760c931abe8345a4a0eb306c36e9ed8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28881 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ../data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8004ee15bc44eb695d4648158e68861",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1648877 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ../data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7212cdcc7f0241c2bfd553a062da724f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4542 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ../data\\MNIST\\raw\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torchvision import transforms, datasets\n",
    "transform=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,)),\n",
    "    ])\n",
    "\n",
    "dataset_train = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "\n",
    "dataset_test = datasets.MNIST('../data', train=False, download=True, transform=transform)\n",
    "\n",
    "# This is a useful function that I didn't know about before\n",
    "first_10k = list(range(0, 10000))\n",
    "dataset_transfer = torch.utils.data.Subset(dataset_train, first_10k)\n",
    "\n",
    "batch_size = 32\n",
    "num_workers = 4\n",
    "transfer_loader = torch.utils.data.DataLoader(dataset_transfer,batch_size=batch_size,num_workers=num_workers)\n",
    "test_loader = torch.utils.data.DataLoader(dataset_test,batch_size=batch_size,num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fe32ec",
   "metadata": {},
   "source": [
    "Here's a function that runs the big model in test mode and provides the number of correct examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8937d3ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9833 -> 98.33%\n"
     ]
    }
   ],
   "source": [
    "def test(model,test_loader):\n",
    "    correct = 0\n",
    "    counter = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data,target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            data = data.reshape(data.shape[0],-1)\n",
    "            logits = model(data)\n",
    "            pred = logits.argmax(dim=1,keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            counter += batch_size\n",
    "    return correct\n",
    "\n",
    "num_correct = test(big_model,test_loader)\n",
    "print( \"{} -> {:.2f}%\".format( num_correct, num_correct/1e4 * 100.))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d3c34a",
   "metadata": {},
   "source": [
    "We find that the big model gets 167 examples wrong (not quite as good as the Hinton paper, but who cares). \n",
    "\n",
    "Now we would like to perform knowledge distillation by training a smaller model to approximate the larger model's performance on the transfer set.  First, let's build a smaller model.  You may use whatever architecture you choose, but I found that using two hidden layers, each with 200 units along with ReLU activations (and no regularization at all) worked fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f6883202",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SmallNet(\n",
       "  (linear_1): Linear(in_features=784, out_features=200, bias=True)\n",
       "  (linear_2): Linear(in_features=200, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SmallNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SmallNet, self).__init__()\n",
    "        # Build a SmallNet\n",
    "        self.l1 = torch.nn.Linear(28**2,200)\n",
    "        self.l2 = torch.nn.Linear(200,10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Don't forget to put the right operations here too!\n",
    "        x = self.l1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.l2(x)\n",
    "        return x\n",
    "    \n",
    "small_model = SmallNet()\n",
    "small_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895fe301",
   "metadata": {},
   "source": [
    "**To establish a baseline performance level, train the small model on the transfer set**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "feac05a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(1.2626)\n",
      "1 tensor(1.1937)\n",
      "2 tensor(1.1214)\n",
      "3 tensor(1.0694)\n",
      "4 tensor(1.0307)\n",
      "5 tensor(0.9929)\n",
      "6 tensor(0.9606)\n",
      "7 tensor(0.9316)\n",
      "8 tensor(0.9073)\n",
      "9 tensor(0.8826)\n",
      "10 tensor(0.8567)\n",
      "11 tensor(0.8310)\n",
      "12 tensor(0.8051)\n",
      "13 tensor(0.7834)\n",
      "14 tensor(0.7644)\n",
      "15 tensor(0.7460)\n",
      "16 tensor(0.7274)\n",
      "17 tensor(0.7076)\n",
      "18 tensor(0.6774)\n",
      "19 tensor(0.6501)\n",
      "20 tensor(0.6294)\n",
      "21 tensor(0.6140)\n",
      "22 tensor(0.6016)\n",
      "23 tensor(0.5909)\n",
      "24 tensor(0.5797)\n",
      "25 tensor(0.5709)\n",
      "26 tensor(0.5648)\n",
      "27 tensor(0.5573)\n",
      "28 tensor(0.5499)\n",
      "29 tensor(0.5422)\n",
      "30 tensor(0.5354)\n",
      "31 tensor(0.5269)\n",
      "32 tensor(0.5166)\n",
      "33 tensor(0.5066)\n",
      "34 tensor(0.4993)\n",
      "35 tensor(0.4931)\n",
      "36 tensor(0.4877)\n",
      "37 tensor(0.4861)\n",
      "38 tensor(0.4794)\n",
      "39 tensor(0.4749)\n",
      "40 tensor(0.4697)\n",
      "41 tensor(0.4654)\n",
      "42 tensor(0.4614)\n",
      "43 tensor(0.4529)\n",
      "44 tensor(0.4463)\n",
      "45 tensor(0.4375)\n",
      "46 tensor(0.4302)\n",
      "47 tensor(0.4243)\n",
      "48 tensor(0.4187)\n",
      "49 tensor(0.4137)\n"
     ]
    }
   ],
   "source": [
    "# I'm giving you this training function: you'll need to modify it below to do knowledge distillation\n",
    "def train(model,train_loader,n_epochs):\n",
    "    optimizer = torch.optim.Adam(model.parameters(),1e-3)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    model.train()\n",
    "    for epoch in range(n_epochs):\n",
    "        avg_l = 0.0\n",
    "        counter = 0\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            data = data.reshape(data.shape[0],-1)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(data)\n",
    "            L = loss_fn(logits,target)\n",
    "            L.backward()\n",
    "            optimizer.step()\n",
    "            with torch.no_grad():\n",
    "                avg_l += L\n",
    "                counter += 1\n",
    "        print(epoch,avg_l/counter)\n",
    "\n",
    "train(small_model,transfer_loader,50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a60a05",
   "metadata": {},
   "source": [
    "**Evaluate the small model on the test set, and comment on its accuracy relative to the big model.**  As you might expect, the performance is relatively worse.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5f760c43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8401 -> 84.01%\n"
     ]
    }
   ],
   "source": [
    "num_correct = test(small_model,test_loader)\n",
    "print( \"{} -> {:.2f}%\".format( num_correct, num_correct/1e4 * 100.))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6343d3eb",
   "metadata": {},
   "source": [
    "**The primary task of this notebook is now as follows: create a new training function similar to \"train\" above, but instead called \"distill\".**  \"distill\" should perform knowledge distillation as outlined in this week's paper.  It should accept a few additional arguments compared to train, namely the big model, the temperature hyperparameter, and a hyperparameter $\\alpha$ that weights the relative magnitude of the soft target loss and the hard target loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "206a7384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.7858387231826782\n",
      "1 1.6275298595428467\n",
      "2 1.579005479812622\n",
      "3 1.5460394620895386\n",
      "4 1.5232347249984741\n",
      "5 1.5070286989212036\n",
      "6 1.4955511093139648\n",
      "7 1.487117052078247\n",
      "8 1.4802067279815674\n",
      "9 1.4749900102615356\n",
      "10 1.4710147380828857\n",
      "11 1.4677759408950806\n",
      "12 1.4649893045425415\n",
      "13 1.4625779390335083\n",
      "14 1.4608081579208374\n",
      "15 1.4589192867279053\n",
      "16 1.4575626850128174\n",
      "17 1.4562504291534424\n",
      "18 1.455114483833313\n",
      "19 1.4541563987731934\n",
      "20 1.4532109498977661\n",
      "21 1.4524013996124268\n",
      "22 1.4516109228134155\n",
      "23 1.450930118560791\n",
      "24 1.4503288269042969\n",
      "25 1.4497402906417847\n",
      "26 1.4492292404174805\n",
      "27 1.4487626552581787\n",
      "28 1.448309063911438\n",
      "29 1.4479272365570068\n",
      "30 1.4475839138031006\n",
      "31 1.4472436904907227\n",
      "32 1.4469393491744995\n",
      "33 1.4466742277145386\n",
      "34 1.4464104175567627\n",
      "35 1.4461771249771118\n",
      "36 1.4459580183029175\n",
      "37 1.4457341432571411\n",
      "38 1.4455450773239136\n",
      "39 1.445366621017456\n",
      "40 1.4452158212661743\n",
      "41 1.4450560808181763\n",
      "42 1.4449042081832886\n",
      "43 1.4447740316390991\n",
      "44 1.444637417793274\n",
      "45 1.4445123672485352\n",
      "46 1.4443951845169067\n",
      "47 1.4442722797393799\n",
      "48 1.4441680908203125\n",
      "49 1.4440782070159912\n"
     ]
    }
   ],
   "source": [
    "distilled_model = SmallNet()\n",
    "distilled_model.to(device)\n",
    "\n",
    "# The body of this method is currently copied verbatim from the train method above: \n",
    "# you will need to modify it to utilize the big_model, temperature, and alpha values \n",
    "# to perform knowledge distillation\n",
    "def distill(small_model,big_model,T,alpha,transfer_loader,n_epochs):\n",
    "    F = torch.nn.functional\n",
    "    optimizer = torch.optim.Adam(small_model.parameters(),1e-3)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    small_model.train()\n",
    "    for epoch in range(n_epochs):\n",
    "        avg_l = 0.0\n",
    "        counter = 0\n",
    "        for batch_idx, (data, target) in enumerate(transfer_loader):\n",
    "            \n",
    "            # load and prep data\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            data = data.reshape(data.shape[0],-1)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # eval data on both models\n",
    "            logits_small = small_model(data)\n",
    "            logits_big   = big_model(data)\n",
    "\n",
    "            # do softmax with Temp\n",
    "            y      = logits_small\n",
    "            y_t    = logits_small/T\n",
    "            y_soft = F.softmax(logits_big/T, dim=1)\n",
    "            \n",
    "            # loss of logits of small model to soft targets\n",
    "            L1 = loss_fn( y_t,y_soft )\n",
    "            # loss on logits of  small model to hard targets\n",
    "            L2 = loss_fn( y,target )\n",
    "\n",
    "            # weighted avg of the two losses\n",
    "            L_wavg = (1-alpha)*L1 + alpha*L2\n",
    "            \n",
    "            # gradient on avg loss\n",
    "            L_wavg.backward()\n",
    "            # backprop\n",
    "            optimizer.step()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                avg_l += L_wavg\n",
    "                counter += 1\n",
    "        print(epoch,(avg_l/counter).item())\n",
    "    \n",
    "T = 20\n",
    "alpha = 1e-1\n",
    "distill(distilled_model,big_model,T,alpha,transfer_loader,50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8bdbbc",
   "metadata": {},
   "source": [
    "**Finally, test your distilled model (on the test set) and describe how it performs relative to both big and small models.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "70ad6e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9650 -> 96.50%\n"
     ]
    }
   ],
   "source": [
    "num_correct = test(distilled_model,test_loader)\n",
    "print( \"{} -> {:.2f}%\".format( num_correct, num_correct/1e4 * 100.))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
